# Mini-Lab 2: Simulate a Mobile Robot with a Depth Camera in Gazebo and Visualize in RViz

This mini-lab combines your knowledge of Gazebo physics, sensor simulation, and ROS 2 communication to create a mobile robot with a depth camera in a simulated environment. You will then visualize the robot's state and sensor data in RViz.

## Learning Objectives

*   Integrate a depth camera sensor into a robot model in Gazebo.
*   Launch a Gazebo simulation with a custom robot.
*   Visualize robot model and depth camera data in RViz.
*   Reinforce ROS 2 topic communication for sensor data.

## Scenario

You will create a simple mobile robot with a depth camera attached. The robot will be spawned in a Gazebo world, and its depth camera data will be published to a ROS 2 topic. You will then use RViz to view the robot model and the point cloud data generated by the depth camera.

## Task Breakdown

1.  **Create a ROS 2 Package**:
    *   If you haven't already, create an `ament_cmake` or `ament_python` package (e.g., `mobile_robot_description`) in your ROS 2 workspace. This package will hold your robot's model files and launch files.

2.  **Define the Mobile Robot (URDF/SDF)**:
    *   Create a simple mobile robot model (e.g., a differential drive base) using URDF.
    *   Integrate a depth camera sensor (similar to the example in `simulating_sensors.md`) into your robot's URDF. Ensure the depth camera plugin (`libgazebo_ros_depth_camera.so`) is correctly configured to publish RGB, depth, and point cloud data to ROS 2 topics (e.g., `/camera/depth/image_raw`, `/camera/depth/points`).
    *   Place this URDF file in your package (e.g., `mobile_robot_description/urdf/mobile_robot.urdf`).

3.  **Create a Gazebo World File (Optional but Recommended)**:
    *   Create a simple Gazebo world file (e.g., `mobile_robot_description/worlds/empty.world`) with a ground plane and perhaps a few simple obstacles for the depth camera to detect.

4.  **Create a Launch File (`launch_mobile_robot.launch.py`)**:
    *   Create a Python launch file within your `mobile_robot_description` package (e.g., `mobile_robot_description/launch/launch_mobile_robot.launch.py`).
    *   This launch file should:
        *   Start Gazebo (empty world or your custom world).
        *   Start `robot_state_publisher` to publish your robot's URDF to ROS 2.
        *   Spawn your mobile robot model into Gazebo using `spawn_entity.py`.
        *   (Optional but recommended) Start RViz 2 to visualize the robot and sensor data. You may need to create an RViz config file.

## Execution and Verification

1.  **Build your Workspace**: Navigate to your ROS 2 workspace root and run `colcon build`.
2.  **Source your Workspace**: `source install/setup.bash`.
3.  **Launch the System**: `ros2 launch mobile_robot_description launch_mobile_robot.launch.py`.
4.  **Observe in Gazebo**: Your mobile robot should appear in Gazebo with its depth camera.
5.  **Observe in RViz**:
    *   Add a `RobotModel` display in RViz to see your robot's structure.
    *   Add a `PointCloud2` display and subscribe to the depth camera's point cloud topic (e.g., `/camera/depth/points`). You should see the point cloud generated by the simulated depth camera.
    *   Add an `Image` display for `/camera/depth/image_raw` to see the depth image.
6.  **Verify ROS 2 Topics**: In a new terminal, check for the depth camera topics:
    ```bash
    ros2 topic list
    ros2 topic echo /camera/depth/points # or image_raw
    ```
    You should see data flowing on these topics.

## Self-Reflection Questions

*   How would you add more sensors (e.g., LiDAR, IMU) to your mobile robot?
*   What happens if you move an object in Gazebo? How is that reflected in RViz?
*   How would you control the robot's movement in this simulated environment?

This mini-lab provides hands-on experience in building a simulated robot with perception capabilities, bridging the gap between robot description, simulation, and ROS 2 visualization.